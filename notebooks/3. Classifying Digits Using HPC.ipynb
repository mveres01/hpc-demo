{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Classifying Digits using HPC\n",
    "---\n",
    "\n",
    "In the previous exercises, we introduced the Compute Canada ecosystem, and initialized our Python environment with the Anaconda package manager, and PyTorch deep learning framework. In this document, we'll demonstrate how to use some of the more common job-scheduling commands for running programs within Compute Canada, and provide a practical example by training a deep, 7-layer convolutional neural network (CNN) to classify the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset.\n",
    "\n",
    "The following commands will be covered:\n",
    "\n",
    "* [salloc](https://slurm.schedmd.com/salloc.html): obtain a Slurm job allocation. This is useful for obtaining an interactive node\n",
    "* [sbatch](https://slurm.schedmd.com/sbatch.html): submits a batch script to slurm\n",
    "* [squeue](https://slurm.schedmd.com/squeue.html): view job information in the slurm queue\n",
    "* [sacct](https://slurm.schedmd.com/sacct.html): shows information on recently completed / running jobs\n",
    "* [scancel](https://slurm.schedmd.com/scancel.html): used to cancel jobs you no longer need or want\n",
    "\n",
    "\n",
    "## About MNIST\n",
    "---\n",
    "\n",
    "MNIST is an extremely popular dataset for testing machine learning algorithms. MNIST dataset is a dataset of hand-written numbers between 0-9, that exist as 28 x 28 pixel images. There are 60,000 training examples and 10,000 testing examples. The goal of the dataset is to learn to predict the correct digit class when shown an image. \n",
    "\n",
    "An example of the dataset (64 samples) can be seen below.\n",
    "\n",
    "<img src=\"images/mnist.png\" width=\"350\">\n",
    "\n",
    "\n",
    "## About the Model\n",
    "---\n",
    "\n",
    "We will be training the following network:\n",
    "\n",
    "<img src=\"images/network.png\" width=\"400\">\n",
    "\n",
    "Which is configured as follows: \n",
    "\n",
    "* 2 convolution layers  (10 filters, kernel size 3x3)\n",
    "* max-pooling layer (2x)\n",
    "* 2 convolution layers  (10 filters, kernel size 3x3)\n",
    "* max-pooling layer (2x)\n",
    "* fully-connected   (64 neurons)\n",
    "* fully-connected   (64 neurons)\n",
    "* fully-connected   (10 neurons)\n",
    "\n",
    "## More Resources\n",
    "---\n",
    "\n",
    "* SHARCNet has an incredible [wiki](https://www.sharcnet.ca/help/index.php/Main_Page) on how to use their resources\n",
    "* [The Stanford CS231 class](http://cs231n.github.io/) is an excellent resource for learning more about deep learning and convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. Getting Started\n",
    "---\n",
    "\n",
    "__NOTE:__ A reservation was made for several SHARCNet nodes, to ensure all participants would have access to the resources during the workshop period. As such, there are several \"reservation=\" lines throughout the code that are not normally required to be present. If you are following this guide outside of the workshop hours, be sure to remove these lines or your job submissions may not run.\n",
    "\n",
    "\n",
    "## 1.1. Task: Clone the Repository\n",
    "---\n",
    "\n",
    "Log in to the _graham_ cluster:\n",
    "\n",
    "```shell\n",
    "ssh graham.sharcnet.ca\n",
    "```\n",
    "\n",
    "Once logged in, download the code repository we'll be using for the workshop. We will try and work from the _scratch_ directory as much as possible. Recall that the scratch directory is __not__ backed up (i.e. if you delete a file, it's gone forever), and files that haven't been used in over 60 days are automatically deleted. \n",
    "\n",
    "* If this is your first time logging in to Compute Canada, your scratch directory may not be initialized yet (give it a few hours); instead, work from your _project_ directory.\n",
    "\n",
    "```shell\n",
    "cd /scratch/<username>\n",
    "git clone https://github.com/mveres01/hpc-demo\n",
    "cd hpc-demo```\n",
    "\n",
    "\n",
    "## 1.2. Task: Download the Data\n",
    "---\n",
    "\n",
    "From the login nodes, download the data we'll use by running the download script. Note that the login nodes are the only ones with access to the internet; even if you were to transfer data from your compute to SHARCNet, you would do so through the login nodes. \n",
    "\n",
    "```shell\n",
    "python download.py\n",
    "```\n",
    "\n",
    "This will download the MNIST dataset, pre-process it, and store it in a folder called data/. \n",
    "\n",
    "### 1.2.1. A Quick Peek into download.py\n",
    "---\n",
    "\n",
    "A quick look inside the file download.py shows the following lines of code:\n",
    "\n",
    "```python\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "data_dir = 'data'\n",
    "if not os.path.exists(data_dir):\n",
    "     os.makedirs(data_dir)\n",
    "\n",
    "data = datasets.MNIST(data_dir, train=True, download=True)\n",
    "data = datasets.MNIST(data_dir, train=False, download=True)\n",
    "```\n",
    "\n",
    "The PyTorch framework that we installed contains pre-built functions for downloading and standardizing data. In this case, it will save the processed data in a single file called test.pt and training.pt. Notice that although there are 60,000 images in the training set, it only occupies ~46MB of disk space, which is often small enough that we can load everything to memory when we perform our training. There are a variety of other methods that can be used in cases where data won't fit in memory, such as saving each image independently as its own .JPG, and learning from a batch of data at a time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. Task: Submitting an Interactive Job\n",
    "---\n",
    "\n",
    "Request an interactive node: we'll use a single GPU for 2 hours\n",
    "\n",
    "\n",
    "```shell\n",
    "salloc --time=2:0:0 --gres=gpu:1 --mem=6000\n",
    "```\n",
    "\n",
    "You should see the following output if successful:\n",
    "\n",
    "```\n",
    "salloc: Pending job allocation 6436997\n",
    "salloc: job 6436997 queued and waiting for resources\n",
    "salloc: job 6436997 has been allocated resources\n",
    "salloc: Granted job allocation 6436997\n",
    "salloc: Waiting for resource configuration\n",
    "salloc: Nodes gra984 are ready for job\n",
    "```\n",
    "\n",
    "The console should have changed from \"@gra-login1\" to the node that's been allocated to you (e.g. to \"@gra984\"). Now that the resource is yours, you are able to ssh _into_ the node using another terminal. If you want. For example, if you were granted node 984 as in this example, you would type:\n",
    "\n",
    "```shell\n",
    "ssh gra984\n",
    "```\n",
    "\n",
    "__NOTE__ that you are _not_ able to ssh into nodes that the scheduler has not assigned you. You can try, but it won't work!\n",
    "\n",
    "## 2.1. Task: Check GPU Resources\n",
    "---\n",
    "\n",
    "To verify that you have been granted access to one GPU, enter the following command in the terminal:\n",
    "\n",
    "```shell\n",
    "nvidia-smi\n",
    "```\n",
    "\n",
    "SHARCNet uses Nvidia graphics cards; the \"smi\" part of the above command stands for _system management interface_. You should see the following:\n",
    "\n",
    "<img src=\"images/nvidia-smi.PNG\" width=\"600\">\n",
    "\n",
    "You can see that we've been allocated a Tesla P100 that's currently drawing 26W of power, and is using 0 bytes of its 12 GB limit. As you run programs on the GPU, the power and memory usage will steadily increase. _Using interactive nodes is a good way to estimate how much memory your models will require, before you go off and submit non-interactive jobs._ \n",
    "\n",
    "\n",
    "## 2.2. Task: Check Job Status\n",
    "---\n",
    "\n",
    "Once your request for a resource has been granted, the job scheduler will immediately begin counting down your remaining time. For interactive jobs, this means that even while you're not running a program, your time continuously decreases. \n",
    "\n",
    "You can view your jobs status with the \"squeue\" command:\n",
    "\n",
    "```shell\n",
    "squeue\n",
    "```\n",
    "\n",
    "To filter the jobs to only show *your* jobs, pass it the \"-u\" flag and specify your username:\n",
    "\n",
    "```shell\n",
    "squeue -u <username>\n",
    "```\n",
    "\n",
    "## 2.3. Task: Running a Model\n",
    "---\n",
    "\n",
    "Now that we are on an interactive node, we need to initialize our workspace. Activate the Python environment we wish to use.\n",
    "\n",
    "```shell\n",
    "source activate pytorch4\n",
    "```\n",
    "\n",
    "Run the \"main.py\" file. This file contains the specification for our CNN and provides a method for iterating over our dataset.\n",
    "\n",
    "```\n",
    "python main.py\n",
    "```\n",
    "\n",
    "You should see something like the following:\n",
    "\n",
    "```\n",
    "Epoch 0 accuracy: 0.0974, took: 45.3128s\n",
    "Epoch 1 accuracy: 0.1135, took: 44.1006s\n",
    "Epoch 2 accuracy: 0.1135, took: 44.1928s\n",
    "Epoch 3 accuracy: 0.1135, took: 44.1635s\n",
    "Epoch 4 accuracy: 0.5687, took: 44.1864s\n",
    "Epoch 5 accuracy: 0.8960, took: 44.2826s\n",
    "```\n",
    "\n",
    "Note the time it takes to complete a full training epoch -- where the network has seen every sample _EPOCH_ times, and updated its weight based on the error it has made. \n",
    "\n",
    "\n",
    "## 2.4. Task: Running a Model with Cuda\n",
    "---\n",
    "\n",
    "Although we've been given access to a GPU model, it turns out the code has not been using it. This could have been diagnosed by running the \"nvidia-smi\" command on a seperate screen to view the usage, while the program was running.\n",
    "\n",
    "In order to use the GPU, the code also needs to support it. PyTorch does, so converting the code to use the GPU is a matter of a couple of statements that moves the network and data to the GPU, before data gets passes through the network. How to write code to do this is outside the scope of this workshop -- but here we have enabled GPU capabilities by specifying the --use-cuda flag:\n",
    "\n",
    "```\n",
    "python main.py --use-cuda\n",
    "```\n",
    "\n",
    "Things should move much faster now.\n",
    "\n",
    "```\n",
    "Epoch 0 accuracy: 0.0974, took: 6.6521s\n",
    "Epoch 1 accuracy: 0.1135, took: 6.5516s\n",
    "Epoch 2 accuracy: 0.1135, took: 6.6110s\n",
    "Epoch 3 accuracy: 0.1135, took: 6.5932s\n",
    "Epoch 4 accuracy: 0.5674, took: 6.5807s\n",
    "Epoch 5 accuracy: 0.8966, took: 6.6388s\n",
    "Epoch 6 accuracy: 0.9142, took: 6.5820s\n",
    "Epoch 7 accuracy: 0.9266, took: 6.6599s\n",
    "Epoch 8 accuracy: 0.9533, took: 6.5851s\n",
    "Epoch 9 accuracy: 0.9613, took: 6.6390s\n",
    "```\n",
    "\n",
    "## 2.5. Section Takeaway\n",
    "\n",
    "Running an interactive node is similar to how you would run a model on your local workstation, wth the main difference being that you must make a request for resources, and then operate within the provided time and memory constraints. \n",
    "\n",
    "It is also important to understand that with deep learning, the time taken to complete a task is some function of:\n",
    "\n",
    "* The depth of the network\n",
    "* The hardware you are using\n",
    "* The amount of data the network sees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3. Task: Submitting a Batch Job\n",
    "---\n",
    "\n",
    "Interactive jobs work great for when we want to interact with code, but there are a lot of time where we just want models to run. Within deep learning, a common situation where this is encountered is in trying to tune _hyperparameters_ of your model. Here, you want to quickly test out different combinations of parameters, and report the configuration that achieves the best result. \n",
    "\n",
    "When we don't need to interact with code, we can submit a request for a non-interactive job to the scheduler. Non-interactive jobs differ from interactive jobs in the following ways (note: non-exhaustive):\n",
    "\n",
    "1. To request an interactive job, we used the command \"salloc\", and specified all constraints on a single line. Non-interactive \"batch\" jobs are achieved using the command \"sbatch\" in conjunction with a configuration file. \n",
    "2. Once an interactive job has been assigned, the amount of time you have left on a resource immediately begins to decrease. This means that if you've been granted access to a node but are not running code, then your time will still decrease. \n",
    "3. Output was written directly to the screen. In the non-interactive version, output from each job is instead written to a file. \n",
    "\n",
    "Non-interactive jobs are usually achieved by using the command \"sbatch\" in conjunction with a configuration file. This file is used to outline our job (and program) constraints. For example, the submit_job.sh script in the project folder contains the following lines:\n",
    "\n",
    "```shell\n",
    "#!/bin/bash\n",
    "#SBATCH --gres=gpu:1                  # Number of GPUs (per node)\n",
    "#SBATCH --mem=4000M                   # memory (per node)\n",
    "#SBATCH --time=0-00:10                # time (DD-HH:MM)\n",
    "#SBATCH --output=slurm-%j.out         # output filename pattern; j == jobid\n",
    "source activate pytorch4\n",
    "python main.py --no-progress\n",
    "```\n",
    "\n",
    "If the code or program that you have developed accepts command-line arguments (e.g. the --use-cuda flag from the previous section), they can get passed in by specifying them on the line beginning with \"python\". To submit a job using this configuration, in the terminal type the following:\n",
    "\n",
    "```shell\n",
    "sbatch submit_job.sh\n",
    "```\n",
    "\n",
    "You should receive confirmation that your job was submitted. You can check its status using the \"squeue -u <username>\" command, and after it begins to run, you should see a file called slurm-xxxxxx.out that appears in the current workspace folder. As the program executes, it will periodically write to this file; to get a quick glimpse of its contents, enter the following on the command line:\n",
    "\n",
    "```shell\n",
    "cat slurm-*.out\n",
    "```\n",
    "\n",
    "## 3.1. Task: Cancelling a Job\n",
    "---\n",
    "\n",
    "The above file was run without CUDA being enabled, so it is occupying a GPU resource, without actually running on the GPU. Let's cancel the job. In the terminal, type: \n",
    "\n",
    "```shell\n",
    "squeue -u <username>\n",
    "```\n",
    "\n",
    "to find the job id. Once you have it, type:\n",
    "\n",
    "```shell\n",
    "cancel <jobid>\n",
    "```\n",
    "\n",
    "If successful, when you type the squeue command again, the job should no longer be in the scheduler. \n",
    "\n",
    "## 3.2. Task: Submit a Batch Job with CUDA flag\n",
    "\n",
    "Modify the batch file to submit the job with the '--use-cuda' flag. Sorry - No help this time :)\n",
    "\n",
    "\n",
    "## 3.3. Email Notifications on Job Updates\n",
    "---\n",
    "\n",
    "If you want notifications for when your job starts, you can specify your email address in the configuration file before running the sbatch command. This is the preferred method to constantly spamming \"squeue\" to see when a job starts, as it puts less stress on the job scheduler. To achieve this, the code below adds two lines that enables an email to be sent to &lt;email address&gt;\n",
    "\n",
    "```shell\n",
    "#!/bin/bash\n",
    "#SBATCH --gres=gpu:1                  # Number of GPUs (per node)\n",
    "#SBATCH --mem=4000M                   # memory (per node)\n",
    "#SBATCH --time=0-00:30                # time (DD-HH:MM)\n",
    "#SBATCH --output=slurm-%j.out         # output filename pattern; j == jobid\n",
    "#SBATCH --mail-user=<email address>\n",
    "#SBATCH --mail-type=ALL\n",
    "python main.py\n",
    "```\n",
    "\n",
    "See [here](https://docs.computecanada.ca/wiki/Running_jobs#Monitoring_jobs) for more details. \n",
    "\n",
    "<img src=\"images/slurm_email.PNG\" width=\"900\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 4. Task: Transferring Data\n",
    "---\n",
    "\n",
    "Running the \"main.py\" file said our accuracy was fairly high -- often over 90%. But what does some of the predictions look like? \n",
    "\n",
    "The code has been saving a small snapshot of predictions on every epoch to the _images/_ folder. Let's retrieve some of the data\n",
    "\n",
    "## 4.1 Windows\n",
    "---\n",
    "\n",
    "Start the program _psftp_. On the command line, type:\n",
    "\n",
    "```shell\n",
    "open graham.computecanada.ca\n",
    "```\n",
    "\n",
    "and enter your username and password. Next, move to the project directory. If you were able to work in the _scratch/_ folder, type the following:\n",
    "\n",
    "```shell\n",
    "cd /scratch/<username>hpc-demo\n",
    "```\n",
    "\n",
    "Next, retrieve the images folder. This is done using the \"mget\" command, and specifying the recursive \"-r\" flag as follows:\n",
    "\n",
    "```shell\n",
    "mget -r images\n",
    "```\n",
    "\n",
    "This will download the images to wherever your current working directory for the psftp program is. You can see the contents of your local directory using the command \"!dir\", and see the contents of the remote directory using the command \"dir\". \n",
    "\n",
    "## 4.2 Linux\n",
    "---\n",
    "\n",
    "There are a lot of flavours to retrieving files with Linux. \n",
    "\n",
    "### 4.2.1. Secure Copy Protocol\n",
    "---\n",
    "\n",
    "One of the easiest is to use the secure-copy protocol (\"scp\"). Open a terminal and type:\n",
    "\n",
    "```shell\n",
    "scp -r <username>@graham.sharcnet.ca:/scratch/<username>/hpc-images .\n",
    "```\n",
    "\n",
    "Lets break it down:\n",
    "\n",
    "* scp -- protocol\n",
    "* -r  -- recursive copy\n",
    "* <username>@graham.sharcnet.ca -- path to the remote node\n",
    "* :/scratch/&lt;username&gt/hpc-images -- destination of file on the remote node\n",
    "* .  -- copy the files to the current directory\n",
    "\n",
    "### 4.2.2. Rsync\n",
    "\n",
    "Another common protocol for copying files is _rsync_. Try the following:\n",
    "\n",
    "rsync -e ssh &lt;username&gt;@graham.sharcnet.ca:/scratch/&lt;username&gt;hpc-images .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 5. Extra Task: Optimizing Network Performance\n",
    "\n",
    "Try playing with the network a little bit. While you have seen that it can accept --use-cuda and --no-progress flaags, it also accepts a range of other ones, including:\n",
    "\n",
    "--seed, --epochs, --lr, --momentum, --batch, and --optimizer.\n",
    "\n",
    "These (minus \"seed\") are known as model _hyperparameters_. Changing them will change the performance of your trained model. Try either starting an interactive job and changing these values (e.g. python main.py --lr=0.001), or  by adding them to the configuration file and submitting a batch job. How accurate can you get the network to be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TODO: A More Complicated Job Submission\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Miscellaneous Tips\n",
    "---\n",
    "\n",
    "## Create Checkpoints for your Models While Training\n",
    "---\n",
    "\n",
    "* In some cases, your model may not be able to finish training within the allotted time. It is usually always a good idea to create _checkpoints_ of your training progress, which are snapshots of the model at certain periods of time. Later, you can use these to quickly restore the state of your model when you want to start training again, or want to switch to performing inference.  \n",
    "\n",
    "\n",
    "## Estimating Resources for Running a Job\n",
    "---\n",
    "\n",
    "Each job run on SHARCNet requires (at a minimum) a specification for memory usage and time usage. \n",
    "\n",
    "* Estimating too many resources: the scheduler may have difficulty finding resources to run\n",
    "* Over-estimating limits: the scheduler may have difficulty finding a spot to run your code\n",
    "* Under-estimating limits: your code may crash if it attempts to access resources it doesn't have permission to\n",
    "\n",
    "## Connecting with Special Flags\n",
    "---\n",
    "\n",
    "Interaction with SHARCNet is done through the Linux console. The console is a medium that allows a user to interact with the linux kernel, while simultaneously allowing the kernel to interact with the user through text-based information. This is quite a different environment then one may be used to on OS such as Windows. There are a number of [flags](https://www.freebsd.org/cgi/man.cgi?query=ssh&sektion=1) that can be entered alongside the SSH \n",
    "command, that will control how the connection is established. A common flag to include is the \"-Y\" flag, which performs X11 forwarding that allows programs requiring a graphical display to be run -- for example, text editors such as gVim or emacs. On linux, the command to launch with this functionality is:\n",
    "\n",
    "```ssh -Y graham.sharcnet.ca```\n",
    "\n",
    "To test if the X-window connection was setup properly, try opening the gVim editor by typing 'gvim' on the console. For Windows users, there is an option in Putty that needs to be checked, under: Connection > SSH > X11 > Enable X11 Forwarding. Additionally, a Windowing application such as Xming that will host the windows\n",
    "\n",
    "<p float=\"left\">\n",
    "<img src=\"images/putty-x11.PNG\" width=\"300\"/> <img src=\"images/putty-x11-xlaunch.PNG\" width=\"300\"/> \n",
    "</p>\n",
    "\n",
    "## Interactive Nodes are Useful for Debugging\n",
    "---\n",
    "\n",
    "Make sure your code is free of bugs. When you submit code to the scheduler, the scheduler finds a suitable time and place for your code to run. When your program executes and is found to have a bug, your remaining time for that particular resource will be forfeit, causing you to submit a new job and wait for an available resource again. \n",
    "\n",
    "## Screen\n",
    "---\n",
    "\n",
    "When you are working in a terminal, you are normally tethered to a single session -- once you close the terminal window, your session usually ends. There are, however, several applications that will allow you to maintain your session even when the window is closed, such as ```tmux``` and ```screen```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
